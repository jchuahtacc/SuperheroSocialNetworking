{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superhero Social Networking\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this notebook, we will use Python to do some web crawling and text mining on Wikipedia to create a network of characters in the Marvel Universe. You can run each Python cell by selecting the cell and hitting the `Ctrl-Enter` keys on your keyboard.\n",
    "\n",
    "To begin with, we need a few variables to control the behavior of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "host = 'https://en.wikipedia.org'\n",
    "max_articles = 200\n",
    "max_depth = 3\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining for a character list\n",
    "\n",
    "A Wikipedia articles can lead to a lot of other articles that have nothing to do with Marvel characters. By text mining the Marvel character list articles, we can come up with a list of article URLs belonging to Marvel characters. This takes a while since we *rate limit* page downloads to prevent overloading Wikipedia. Therefore, this code is in a function that can be executed if you uncomment the last line of the code cell. The list has already been saved to the `marvel_urls.txt` text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def textmine_marvel_urls():\n",
    "    import time\n",
    "    slug = '/wiki/List_of_Marvel_Comics_characters:'\n",
    "    sections = ['_0-9']\n",
    "    letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    for char in letters:\n",
    "        sections.append('_' + char)\n",
    "\n",
    "    character_urls = list()\n",
    "\n",
    "    for section in sections:\n",
    "        time.sleep(1)\n",
    "        character_list_url = host + slug + section\n",
    "        print \"Downloading \" + character_list_url\n",
    "        page = PyQuery(url=character_list_url)\n",
    "        hatnotes = page(\".hatnote\")\n",
    "        for hatnote in hatnotes:\n",
    "            noteQuery = PyQuery(hatnote)\n",
    "            if \"Main article\" in noteQuery.text():\n",
    "                link = noteQuery.find(\"a\")\n",
    "                if link:\n",
    "                    character_urls.append(link.attr(\"href\"))\n",
    "\n",
    "    output_file = open('marvel_urls.txt', 'w')\n",
    "\n",
    "    for url in character_urls:\n",
    "        output_file.write(url)\n",
    "        output_file.write(\"\\n\")\n",
    "\n",
    "    output_file.flush()\n",
    "    output_file.close()\n",
    "\n",
    "# textmine_marvel_urls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a list of Marvel character URLs\n",
    "\n",
    "Since we already have a list of Marvel URLs stored in a file, we can load it into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = open('marvel_urls.txt', 'r')\n",
    "character_urls = input_file.readlines()\n",
    "character_urls = [url.strip() for url in character_urls]\n",
    "\n",
    "print str(len(character_urls)) + \" characters total\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Culling the list\n",
    "\n",
    "There are more than 2500 articles related to Marvel characters. Since this network is too large to compute in a reasonable amount of time, let's cut it down to only those that appear in the Marvel Cinematic Universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyquery import PyQuery\n",
    "\n",
    "mcu_article = PyQuery(url=\"https://en.wikipedia.org/wiki/List_of_Marvel_Cinematic_Universe_film_actors\")\n",
    "mcu_html = mcu_article.html()\n",
    "\n",
    "shorter_list = []\n",
    "for url in character_urls:\n",
    "    if url in mcu_html:\n",
    "        shorter_list.append(url)\n",
    "        \n",
    "character_urls = shorter_list\n",
    "print str(len(character_urls)) + \" characters in the Marvel Cinematic Universe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining an Article\n",
    "\n",
    "If we retrieve a Wikipedia article about a character, we need to mine the data from it to see which Marvel Cinematic Universe characters it links to. The `Article` class listed below takes care of these details. It downloads a page given a URL, performs text mining to determine the title of the page, and fills its `links` member with a link to new `Article` objects with Marvel Cinematic Universe articles, to be crawled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyquery import PyQuery\n",
    "\n",
    "\n",
    "class Article:\n",
    "    links = None\n",
    "    depth = 0\n",
    "    from_url = \"\"\n",
    "    url = \"\"\n",
    "    page_contents = None\n",
    "    from_character = None\n",
    "    character = \"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.links = list()\n",
    "        self.depth = 0\n",
    "        self.from_url = \"\"\n",
    "        self.url = \"\"\n",
    "        self.page_contents = None\n",
    "        self.from_character = None\n",
    "        self.character = \"\"\n",
    "        if \"from_url\" in kwargs:\n",
    "            self.from_url = kwargs[\"from_url\"]\n",
    "        if \"url\" in kwargs:\n",
    "            self.url = kwargs[\"url\"]\n",
    "        if \"depth\" in kwargs:\n",
    "            self.depth = kwargs[\"depth\"]\n",
    "        if \"from_character\" in kwargs:\n",
    "            self.from_character  = kwargs[\"from_character\"]\n",
    "        if \"character\" in kwargs:\n",
    "            self.character = kwargs[\"character\"]\n",
    "    \n",
    "    def __str__(self):\n",
    "        result = \"Destination: \" + self.url\n",
    "        if self.character:\n",
    "            result = result = \" (\" + self.character + \")\"\n",
    "        return result\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.url == other.url\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return not self == other\n",
    "            \n",
    "    def loadPage(self):\n",
    "        full_url = host + self.url\n",
    "        self.page_contents = PyQuery(url=full_url)\n",
    "        self.character = self.page_contents(\"h1\").text()       \n",
    "        if self.depth < max_depth:\n",
    "            for a in self.page_contents(\"a\"):\n",
    "                aQuery = PyQuery(a)\n",
    "                href = aQuery.attr(\"href\")\n",
    "                if href and href.startswith(\"/wiki/\") and href in character_urls:\n",
    "                    self.links.append(Article(from_url=self.url, url=href, from_character=self.character, depth=self.depth + 1))\n",
    "            \n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Crawling the Articles on Wikipedia\n",
    "\n",
    "Now we need to crawl the articles and save link information. This `Crawler` class will crawl articles, given a starting point. By default, it chooses Iron Man as a starting point.\n",
    "\n",
    "_*Try this*_\n",
    "\n",
    "Translate the comments in the `crawl_articles` function, in order of their numbering, to create the crawling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import networkx as nx\n",
    "import json\n",
    "from collections import deque\n",
    "\n",
    "class Crawler:\n",
    "    start_article = None\n",
    "    crawl_queue = None\n",
    "    visited = None\n",
    "    data = None\n",
    "    \n",
    "    def __init__(self, start_url = \"/wiki/Iron_Man\"):\n",
    "        self.crawl_queue = deque()\n",
    "        self.visited = list()\n",
    "        self.data = dict()\n",
    "        self.start_article = Article(url=start_url)\n",
    "        self.crawl_queue.append(self.start_article)\n",
    "    \n",
    "    def register_article(self, article):\n",
    "        entry = dict()\n",
    "        entry[\"character\"] = article.character\n",
    "        entry[\"links\"] = list()\n",
    "        for link in article.links:\n",
    "            entry[\"links\"].append(link.url)\n",
    "        self.data[article.url] = entry\n",
    "    \n",
    "    def print_article(self, article):\n",
    "        result = \"Processed \" + article.character\n",
    "        if article.from_character:\n",
    "            result = result + \" (from \" + article.from_character +\")\"\n",
    "        result = result + \" with \" + str(len(article.links)) + \" links\"\n",
    "        print result\n",
    "        \n",
    "    def save_data(self, filename=\"article_data.json\"):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.data, f)\n",
    "        \n",
    "    def crawl_articles(self):       \n",
    "        while len(self.crawl_queue) > 0 and len(self.visited) < max_articles:\n",
    "            time.sleep(2)\n",
    "            # 1. Dequeue the starting article from self.crawl_queue using crawl_queue's popleft() function, and save it as a variable called article\n",
    "            #    Print article\n",
    "\n",
    "            # 2. Call article's loadPage function\n",
    "            #    Print article\n",
    "            #    Register the article (use self.register_article())\n",
    "            #    Add it to the list of visited articles\n",
    "\n",
    "            # 3. For each link in article.links\n",
    "            #        print link\n",
    "            # 4.     if the link is not in self.crawl_queue\n",
    "            #            append it to self.crawl_queue\n",
    "            # 5. print the length of self.crawl_queue. (It should be about 60 links)            \n",
    "            \n",
    "\n",
    "# Uncomment these lines to mine articles and save data\n",
    "crawler = Crawler(start_url=\"/wiki/Iron_Man\")\n",
    "crawler.crawl_articles()\n",
    "crawler.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Matrix\n",
    "\n",
    "Now we need to keep track of connections between characters. We can do this using a weighted Edge Matrix. Simply put, it's a table where the rows are a character, and the columns of each row count the number of connections to the corresponding character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EdgeMatrix:\n",
    "    edge_map = []\n",
    "    url_indexes = dict()\n",
    "    url_names = dict()\n",
    "    urls = list()\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def add_url(self, url):\n",
    "        for row in self.edge_map:\n",
    "            row.append(0)\n",
    "        self.edge_map.append([0 for x in range(1, len(self.edge_map) + 2)])\n",
    "        self.url_indexes[url] = len(self.edge_map) - 1\n",
    "        self.urls.append(url)\n",
    "    \n",
    "    def mark_edge(self, from_item, to_item):\n",
    "        if from_item.url not in self.url_indexes:\n",
    "            self.add_url(from_item.url)\n",
    "        if to_item.url not in self.url_indexes:\n",
    "            self.add_url(to_item.url)\n",
    "        from_index = self.url_indexes[from_item.url]\n",
    "        to_index = self.url_indexes[to_item.url]\n",
    "        self.edge_map[ from_index ][ to_index ] = self.edge_map[ from_index ][ to_index] + 1\n",
    "        \n",
    "        if from_item.url not in self.url_names or not self.url_names[from_item.url]:\n",
    "            self.url_names[from_item.url] = from_item.character\n",
    "        \n",
    "        if to_item.url not in self.url_names or not self.url_names[to_item.url]:\n",
    "            self.url_names[to_item.url] = to_item.character\n",
    "            \n",
    "    def get_name(self, index):\n",
    "        return self.url_names[self.urls[index]]\n",
    "\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating the Edge Matrix\n",
    "\n",
    "Now that we have the link data and some where to store it, we need to fill the Edge Matrix. The `data` dictionary is filled with the article data from the file we saved from web crawling earlier. To populate it, we visit each url in `data` and create `from_article`, which is an instance of an `Article`. Then we re-create an `Article` from each link, and save it as `to_article`. Finally, we register each edge using these two articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = None\n",
    "\n",
    "with open('article_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "edges = EdgeMatrix()\n",
    "\n",
    "for url in data:\n",
    "    from_article = Article(character=data[url][\"character\"], url=url)\n",
    "    for link in data[url][\"links\"]:\n",
    "        to_article = Article(url=link)\n",
    "        edges.mark_edge(from_article, to_article)\n",
    "        \n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Social Network\n",
    "\n",
    "We can create a visual representation of the network from the edges that we saved. Currently, this visualization just tries to connect every edge that exists in our matrix. It's difficult to see anything and isn't visually appealing. Try the following:\n",
    "\n",
    "- Iterate through `graph.edges(data=True)`. This returns a list of tuples `(u, v, d)` where `d` is a dictionary containing the `\"weight\"` of an edge. Save these weights in the list `weights`\n",
    "- Calling `nx.degree(graph)` returns a dictionary of nodes and their degree. Iterate through this list and save the degree in the `degrees` list\n",
    "- Change the `node_color_map` and `edge_color_map` to a color map, such as `plt.cm.Reds`. For a full list of color maps, refer to [this reference page](http://matplotlib.org/examples/color/colormaps_reference.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "graph = nx.DiGraph()\n",
    "edge_map = edges.edge_map\n",
    "length = len(edges.edge_map)\n",
    "\n",
    "weights = list()\n",
    "degrees = list()\n",
    "\n",
    "for row in range(0, length):\n",
    "    for col in range(0, length):\n",
    "        if edge_map[row][col] > 0:\n",
    "            graph.add_edge(row, col, weight=weights[row][col])\n",
    "\n",
    "# Iterate through edge tuples\n",
    "for (u, v, d) in graph.edges(data=True):\n",
    "    print d\n",
    "    # Append the \"weight\" data from d to weights\n",
    "\n",
    "# Iterate through node degrees \n",
    "node_dict = nx.degree(graph):\n",
    "for node in node_dict:\n",
    "    print node_dict[node]\n",
    "    # Append the degree data to degrees\n",
    "\n",
    "    \n",
    "# Try assigning different color maps\n",
    "node_color_map = None\n",
    "edge_color_map = None\n",
    "    \n",
    "if not len(weights):\n",
    "    weights = 'k'\n",
    "    \n",
    "if not len(degrees):\n",
    "    sizes = 300\n",
    "    \n",
    "if len(degrees):\n",
    "    vmin = min(degrees)\n",
    "    vmax = max(degrees)\n",
    "else:\n",
    "    vmin = 0\n",
    "    vmax = 0\n",
    "    \n",
    "if len(weights):\n",
    "    edge_vmin = min(weights)\n",
    "    edge_vmax = max(weights)\n",
    "else:\n",
    "    edge_vmin = 0\n",
    "    edge_vmax = 0\n",
    "    \n",
    "plt.figure(1, figsize=(12, 12))\n",
    "nx.draw_networkx(graph, vmin=vmin, vmax=vmax, with_labels=True,edge_color=weights, node_color=degrees, edge_vmin=edge_vmin, edge_vmax=edge_vmax, edge_cmap=edge_color_map, cmap=node_color_map, arrows=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Network\n",
    "\n",
    "Your eye may be drawn to certain nodes because of the coloring of the nodes. The nodes that are brightest have the highest degrees. They are the most interconnected. Let's look at those node names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interesting = [ 34, 15, 33, 39 ]\n",
    "\n",
    "for node in interesting:\n",
    "    print node, edges.get_name(node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge weights\n",
    "\n",
    "Your eye may also be draw to heavily weighted edges. This illustrates strong connections between characters. Let's look at two nodes with dark red lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interesting = [ 3, 48 ]\n",
    "\n",
    "for node in interesting:\n",
    "    print node, edges.get_name(node)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
